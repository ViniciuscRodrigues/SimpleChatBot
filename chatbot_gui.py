import streamlit as st
from llama_cpp import Llama

# --------------------------------------------------------------------------
# CONFIGURA√á√ÉO DA P√ÅGINA
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="Assistente de Impress√£o 3D",
    page_icon="ü§ñ",
    layout="centered"
)

# --------------------------------------------------------------------------
# 1. CARREGAMENTO DO MODELO (COM CACHE)
# --------------------------------------------------------------------------
# O decorador @st.cache_resource garante que o modelo seja carregado apenas
# uma vez, otimizando o desempenho da aplica√ß√£o.
@st.cache_resource
def load_model():
    """Carrega o modelo LLM e o retorna."""
    try:
        llm = Llama(
            model_path="./Phi-3-mini-4k-instruct-q4.gguf", # <-- Verifique o nome do arquivo
            n_ctx=4096,
            n_gpu_layers=0,
            verbose=False
        )
        return llm
    except Exception as e:
        # Mostra uma mensagem de erro no Streamlit se o modelo n√£o for encontrado
        st.error(f"Erro ao carregar o modelo: {e}", icon="üö®")
        st.stop() # Interrompe a execu√ß√£o do app se n√£o puder carregar o modelo

# Exibe uma mensagem enquanto o modelo est√° sendo carregado
with st.spinner("Carregando o especialista em Impress√£o 3D... Por favor, aguarde."):
    llm = load_model()

# --------------------------------------------------------------------------
# 2. DEFINI√á√ÉO DO CONTEXTO E INICIALIZA√á√ÉO DO CHAT
# --------------------------------------------------------------------------
# Define a persona do chatbot
system_message = (
    "Voc√™ √© um assistente especialista em impress√£o 3D. "
    "Sua miss√£o √© ajudar usu√°rios, desde iniciantes at√© avan√ßados, com suas d√∫vidas. "
    "Responda de forma clara, did√°tica e objetiva. Forne√ßa dicas pr√°ticas sobre filamentos, "
    "configura√ß√µes de fatiadores (slicers), solu√ß√£o de problemas comuns (como stringing, warping) e manuten√ß√£o de impressoras."
)

# T√≠tulo da aplica√ß√£o que aparece na p√°gina
st.title("ü§ñ Assistente de Impress√£o 3D")
st.caption("Pergunte-me qualquer coisa sobre o universo da impress√£o 3D!")

# Inicializa o hist√≥rico da conversa no estado da sess√£o, se ainda n√£o existir.
# Isso garante que o hist√≥rico persista entre as intera√ß√µes do usu√°rio.
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe todas as mensagens do hist√≥rico na interface
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --------------------------------------------------------------------------
# 3. L√ìGICA DE INTERA√á√ÉO DO CHAT
# --------------------------------------------------------------------------
# Cria um campo de entrada de texto no rodap√© da p√°gina.
# O que o usu√°rio digitar aqui ser√° armazenado na vari√°vel 'prompt'.
if prompt := st.chat_input("Qual √© a sua d√∫vida sobre impress√£o 3D?"):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe na tela
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Prepara para gerar a resposta do assistente
    with st.chat_message("assistant"):
        # Mostra um √≠cone de "pensando" enquanto a resposta √© gerada
        with st.spinner("Pensando..."):
            # Prepara a conversa para o modelo, incluindo a instru√ß√£o do sistema
            conversation_for_model = [
                {"role": "system", "content": system_message}
            ] + st.session_state.messages

            # Chama o modelo para obter a resposta
            response = llm.create_chat_completion(
                messages=conversation_for_model,
                max_tokens=500,
                stop=["<|end|>", "<|user|>"],
                temperature=0.7,
                stream=False # Desativar stream para respostas completas
            )

            # Extrai e exibe a resposta
            assistant_message = response['choices'][0]['message']['content'].strip()
            st.markdown(assistant_message)

    # Adiciona a resposta do assistente ao hist√≥rico da conversa
    st.session_state.messages.append({"role": "assistant", "content": assistant_message})